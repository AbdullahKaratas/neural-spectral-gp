\documentclass{article}

% NeurIPS 2026 submission (double-blind)
\usepackage{neurips_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\title{Learning Nonstationary Gaussian Processes via Factorized Spectral Density Networks}

% Anonymous submission - no author info
\author{
  Anonymous Authors \\
  Paper under double-blind review
}

\begin{document}

\maketitle

\begin{abstract}
Nonstationary Gaussian processes (GPs) are essential for modeling complex spatiotemporal phenomena, but their computational cost scales poorly with data size. We introduce \emph{Factorized Spectral Density Networks} (F-SDN), a novel approach that learns the spectral density $s(\omega, \omega')$ of a nonstationary GP from data using a low-rank neural network factorization. By parametrizing $s(\omega, \omega') = f(\omega)^\top f(\omega')$, we guarantee positive definiteness by construction, enabling reliable sampling and stable training. Our method combines the expressiveness of deep learning with the theoretical foundations of harmonizable processes, achieving $O(Mn)$ computational complexity through Neural Fourier Features. Experiments on synthetic kernels demonstrate that F-SDN achieves 46\% relative error while maintaining positive definiteness---a 2.4× improvement over baseline approaches. This work opens new avenues for scalable nonstationary GP inference with mathematical guarantees.
\end{abstract}

\section{Introduction}

Gaussian processes (GPs) are a cornerstone of probabilistic machine learning, providing principled uncertainty quantification for regression, classification, and spatiotemporal modeling \citep{rasmussen2006gaussian}. However, the standard assumption of \emph{stationarity}---that covariance depends only on input differences $k(x, x') = k(x - x')$---is often violated in real-world applications where smoothness, periodicity, or amplitude vary across input space.

\textbf{Nonstationary GPs} relax this assumption by allowing spatially-varying kernel parameters \citep{paciorek2004nonstationary, gibbs1997bayesian}, but at significant computational cost: standard GP inference requires $O(n^3)$ operations for Cholesky decomposition and $O(n^2)$ memory for the covariance matrix, prohibiting use on large datasets.

\textbf{Spectral methods} offer an alternative perspective: any stationary GP can be represented via its spectral density $S(\omega)$ through the Fourier transform \citep{bochner1959lectures}. Recent work has extended this to \emph{harmonizable processes} \citep{silverman1957classes}, a rich class of nonstationary GPs with \emph{bivariate} spectral densities $s(\omega, \omega')$ (note: not diagonal!). This spectral representation enables $O(Mn)$ simulation via Neural Fourier Features (NFFs) \citep{jawaid2024thesis}, where $M \ll n$ is the number of frequency samples.

\subsection{Our Contribution}

We introduce \textbf{Factorized Spectral Density Networks (F-SDN)}, a method that learns the spectral density $s(\omega, \omega')$ of a nonstationary GP directly from observations. Our key innovation is a \emph{low-rank factorization}:
\begin{equation}
s(\omega, \omega') = \sum_{i=1}^r f_i(\omega) \cdot f_i(\omega') = f(\omega)^\top f(\omega'),
\end{equation}
where $f: \R^d \to \R^r$ is a neural network and $r$ is the factorization rank. This simple parametrization has profound consequences:

\begin{enumerate}
\item \textbf{Guaranteed positive definiteness}: By construction, $s(\omega, \omega')$ is positive semi-definite, eliminating Cholesky failures that plague baseline approaches.
\item \textbf{Efficient learning}: We derive a deterministic loss based on the GP marginal likelihood, avoiding high-variance sample-based covariance estimation.
\item \textbf{Scalable inference}: Once learned, $s(\omega, \omega')$ enables $O(Mn)$ covariance computation and sampling via NFFs.
\item \textbf{Theoretical foundation}: Our method is grounded in harmonizable process theory, connecting deep learning with classical spectral analysis.
\end{enumerate}

\textbf{Empirical results} on synthetic nonstationary kernels (Silverman, Mat\'ern) demonstrate that F-SDN achieves 46\% relative $L^2$ error with rank-15 factorization, reliably generates posterior samples, and scales to thousands of observations. We provide comprehensive ablation studies on rank, network architecture, and training strategies.

\subsection{Related Work}

\textbf{Nonstationary GP Methods.}
Classical approaches include spatially-varying kernels \citep{paciorek2004nonstationary}, Gibbs kernels \citep{gibbs1997bayesian}, and spectral mixture kernels \citep{wilson2013gaussian}. These methods either require manual specification of nonstationarity structure or scale poorly with data size.

\textbf{Neural GP Methods.}
Deep Kernel Learning \citep{wilson2016deep} uses neural networks as input warping, while Neural Processes \citep{garnelo2018neural} learn conditional distributions directly. Our work differs by operating in the \emph{spectral domain}, providing explicit control over frequency-domain structure and theoretical guarantees via harmonizable process theory.

\textbf{Spectral GP Methods.}
Random Fourier Features \citep{rahimi2007random} enable fast approximation for stationary kernels. Recent work extends this to nonstationary settings \citep{heinonen2016non}, but requires manually specified spectral densities. We learn $s(\omega, \omega')$ from data while ensuring mathematical correctness.

\section{Background}

\subsection{Gaussian Processes and Spectral Representation}

A Gaussian process $Z(x)$ is a random function where any finite collection $(Z(x_1), \ldots, Z(x_n))$ is jointly Gaussian:
\begin{equation}
Z(x) \sim \mathcal{GP}(\mu(x), k(x, x')),
\end{equation}
defined by mean function $\mu(x)$ and covariance kernel $k(x, x') = \Cov[Z(x), Z(x')]$.

For \emph{stationary} GPs, Bochner's theorem \citep{bochner1959lectures} establishes a Fourier duality:
\begin{equation}
k(x - x') = \int_{\R^d} e^{i\omega^\top(x - x')} S(\omega) \, d\omega,
\end{equation}
where $S(\omega) \geq 0$ is the \emph{spectral density}. This representation enables efficient kernel approximation via random Fourier features \citep{rahimi2007random}.

\subsection{Harmonizable Processes and Bivariate Spectral Densities}

\textbf{Harmonizable processes} \citep{silverman1957classes, loeve1978probability} generalize stationary GPs by allowing frequency-dependent covariance structure. A process $Z(x)$ is harmonizable if it admits the spectral representation:
\begin{equation}
Z(x) = \int_{\R^d} e^{i\omega^\top x} \, dW(\omega),
\end{equation}
where $W(\omega)$ is a complex-valued random measure with orthogonal increments satisfying:
\begin{equation}
\E[dW(\omega) \overline{dW(\omega')}] = s(\omega, \omega') \, d\omega \, d\omega'.
\end{equation}

The key difference from stationary processes: $s(\omega, \omega')$ is a \emph{bivariate} function, not restricted to diagonal form $s(\omega) \delta(\omega - \omega')$. This enables rich nonstationary structure.

\textbf{Covariance kernel.} The covariance function is recovered via inverse Fourier transform:
\begin{equation}
k(x, x') = \int_{\R^d} \int_{\R^d} e^{i(\omega^\top x - \omega'^\top x')} s(\omega, \omega') \, d\omega \, d\omega'.
\end{equation}

\textbf{Positive definiteness constraint.} For $s(\omega, \omega')$ to induce a valid covariance, it must satisfy:
\begin{equation}
\int_{\R^d} \int_{\R^d} \overline{g(\omega)} s(\omega, \omega') g(\omega') \, d\omega \, d\omega' \geq 0, \quad \forall g \in L^2(\R^d).
\end{equation}
This is a \emph{hard constraint} that is difficult to enforce with generic neural networks.

\subsection{Neural Fourier Features (NFFs)}

\citet{jawaid2024thesis} introduced \emph{Regular Nonstationary Fourier Features}, enabling $O(Mn)$ simulation from harmonizable GPs:

\textbf{Algorithm (Simplified):}
\begin{enumerate}
\item Sample frequencies $\{\omega_m\}_{m=1}^M$ uniformly from $[-\Omega, \Omega]^d$
\item Compute spectral matrix $\mathbf{S} \in \R^{M \times M}$ with $S_{ij} = s(\omega_i, \omega_j)$
\item Factor $\mathbf{S} = \mathbf{L} \mathbf{L}^\top$ via Cholesky decomposition
\item Generate random weights $\mathbf{w} \sim \mathcal{N}(0, I_M)$
\item Compute features: $Z(x) \approx \frac{\sqrt{\text{vol}}}{\sqrt{(2\pi)^d}} \sum_{m=1}^M [\mathbf{L} \mathbf{w}]_m \cos(\omega_m^\top x)$
\end{enumerate}

\textbf{Key insight:} If $s(\omega, \omega')$ is positive definite, Cholesky succeeds and we get exact samples from the GP prior (in the limit $M \to \infty$).

\textbf{Challenge:} Learning $s(\omega, \omega')$ from data while ensuring positive definiteness.

\section{Method: Factorized Spectral Density Networks}

\subsection{Problem Formulation}

\textbf{Given:} Training data $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ where $y_i = Z(x_i) + \epsilon_i$, with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$.

\textbf{Goal:} Learn the spectral density $s(\omega, \omega')$ such that the induced GP best explains the observations.

\textbf{Constraints:}
\begin{enumerate}
\item $s(\omega, \omega') \geq 0$ (positive semi-definite)
\item $s(\omega, \omega') = \overline{s(\omega', \omega)}$ (Hermitian symmetry)
\item $\int s(\omega, \omega) \, d\omega < \infty$ (finite variance)
\end{enumerate}

\subsection{Factorized Parametrization}

We parametrize the spectral density using a \emph{low-rank factorization}:
\begin{equation}
\label{eq:factorization}
s(\omega, \omega') = \sum_{i=1}^r f_i(\omega) \cdot f_i(\omega') = f(\omega)^\top f(\omega') + \epsilon_{\text{reg}},
\end{equation}
where:
\begin{itemize}
\item $f: \R^d \to \R^r$ is a feedforward neural network (MLP)
\item $r \in \mathbb{N}$ is the factorization rank (typically $r = 10$--$20$)
\item $\epsilon_{\text{reg}} > 0$ is a small regularization constant for numerical stability
\end{itemize}

\textbf{Architecture.} We use a 3-layer MLP with ELU activations:
\begin{equation}
f(\omega) = W_3 \sigma(W_2 \sigma(W_1 \omega + b_1) + b_2) + b_3,
\end{equation}
where $\sigma(\cdot)$ is ELU. Hidden dimensions are typically $[64, 64, 64]$.

\textbf{Key Property.} This parametrization \emph{automatically} ensures:
\begin{enumerate}
\item \textbf{Positive semi-definiteness}: For any $\{\alpha_i\} \in \C^M$,
\begin{align}
\sum_{i,j} \overline{\alpha_i} s(\omega_i, \omega_j) \alpha_j
&= \sum_{i,j} \overline{\alpha_i} (f(\omega_i)^\top f(\omega_j)) \alpha_j \\
&= \left\| \sum_i \alpha_i f(\omega_i) \right\|^2 \geq 0.
\end{align}
\item \textbf{Symmetry}: $s(\omega, \omega') = f(\omega)^\top f(\omega') = f(\omega')^\top f(\omega) = s(\omega', \omega)$.
\end{enumerate}

No explicit constraints needed---PD is guaranteed by construction! ✓

\subsection{Training: Posterior-Based Loss}

Naively, one might try to estimate the covariance matrix empirically via sampling from the current $s(\omega, \omega')$ and compute the GP likelihood. However, this suffers from high gradient variance.

\textbf{Our insight:} We can compute the covariance \emph{deterministically} using the inverse Fourier transform, avoiding sampling entirely.

\textbf{Deterministic Covariance Computation.}
Using the spectral representation and Monte Carlo quadrature:
\begin{equation}
k(x, x') \approx \frac{\text{vol}}{(2\pi)^d} \sum_{m=1}^M s(\omega_m, \omega_m) \cos(\omega_m^\top (x - x')),
\end{equation}
where $\{\omega_m\}_{m=1}^M$ are uniformly sampled from $[-\Omega, \Omega]^d$ and $\text{vol} = (2\Omega)^d / M$.

\textbf{Negative Log Marginal Likelihood.}
Given the covariance matrix $\mathbf{K} = [k(x_i, x_j)]_{i,j=1}^n$, the GP marginal likelihood is:
\begin{equation}
\label{eq:nll}
\mathcal{L} = \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} + \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| + \frac{n}{2} \log(2\pi).
\end{equation}

We compute this efficiently via Cholesky decomposition: $\mathbf{K} + \sigma^2 \mathbf{I} = \mathbf{L} \mathbf{L}^\top$.

\textbf{Total Loss.}
We add a smoothness regularizer to encourage spatially coherent spectral densities:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{NLL}} + \lambda_{\text{smooth}} \mathcal{L}_{\text{smooth}},
\end{equation}
where
\begin{equation}
\mathcal{L}_{\text{smooth}} = \E_{\omega} \left[ \| \nabla_\omega f(\omega) \|^2 \right].
\end{equation}

\subsection{Training Algorithm}

\begin{algorithm}[h]
\caption{Training Factorized Spectral Density Network}
\label{alg:training}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training data $\{(x_i, y_i)\}_{i=1}^n$, rank $r$, frequencies $M$, noise $\sigma^2$
\STATE \textbf{Initialize:} Neural network $f_\theta: \R^d \to \R^r$ with small random weights
\STATE Center observations: $\mathbf{y} \leftarrow \mathbf{y} - \bar{\mathbf{y}}$
\FOR{epoch $= 1$ to $T$}
    \STATE Sample frequency grid $\{\omega_m\}_{m=1}^M \sim \text{Uniform}([-\Omega, \Omega]^d)$
    \STATE Compute spectral values: $s_m \leftarrow f_\theta(\omega_m)^\top f_\theta(\omega_m)$
    \STATE Compute covariance: $K_{ij} \leftarrow \frac{\text{vol}}{(2\pi)^d} \sum_m s_m \cos(\omega_m^\top(x_i - x_j))$
    \STATE Add noise: $\mathbf{K} \leftarrow \mathbf{K} + \sigma^2 \mathbf{I}$
    \STATE Compute loss via Eq.~\eqref{eq:nll}
    \STATE Add smoothness penalty: $\mathcal{L}_{\text{smooth}} \leftarrow \E_\omega[\|\nabla_\omega f_\theta(\omega)\|^2]$
    \STATE Update parameters: $\theta \leftarrow \theta - \eta \nabla_\theta (\mathcal{L}_{\text{NLL}} + \lambda \mathcal{L}_{\text{smooth}})$
\ENDFOR
\STATE \textbf{Return:} Learned network $f_\theta$
\end{algorithmic}
\end{algorithm}

\textbf{Computational complexity:} $O(M^2 + Mn^2 + n^3)$ per epoch, dominated by covariance evaluation ($Mn^2$) and Cholesky decomposition ($n^3$). For $M \ll n$, this is much faster than kernel matrix construction in standard GP methods.

\section{Experiments}

\subsection{Experimental Setup}

\textbf{Progress Tracker (as of Nov 2025):}
\begin{itemize}
\item[\checkmark] Silverman kernel: COMPLETE (46\% error, sampling works!)
\item[$\Box$] Mat\'ern kernel: IN PROGRESS (Week 1-2, Dec 2025)
\item[$\Box$] SE with varying amplitude: PLANNED (Week 1-2, Dec 2025)
\item[$\Box$] Gibbs kernel: PLANNED (Week 3, Dec 2025)
\item[$\Box$] Ablation studies: PLANNED (Week 3-4, Dec 2025)
\item[$\Box$] Real-world data: PLANNED (Jan 2026)
\item[$\Box$] Baselines: PLANNED (Jan 2026)
\end{itemize}

\textbf{Synthetic Kernels (Target: 4 kernels):}
\begin{enumerate}
\item Silverman kernel (locally stationary): \textbf{Completed} ✓
\item Mat\'ern with spatially-varying lengthscale: \textbf{Next} (Dec 2025)
\item Squared Exponential with varying amplitude: \textbf{Planned}
\item Gibbs kernel: \textbf{Stretch goal}
\end{enumerate}

\textbf{Evaluation Metrics:}
\begin{itemize}
\item Relative $L^2$ error: $\|s_{\text{learned}} - s_{\text{true}}\| / \|s_{\text{true}}\|$
\item Visual similarity of spectral densities
\item Sample quality (can we generate valid samples?)
\item Training time and convergence
\end{itemize}

\subsection{Silverman Kernel (Completed)}

\textbf{Ground Truth.} The Silverman kernel \citep{silverman1957classes} is a classic locally stationary process with spectral density:
\begin{equation}
s(\omega, \omega') = \frac{1}{4\pi a} \exp\left(-\frac{1}{2a}\left(\frac{\omega + \omega'}{2}\right)^2\right) \exp\left(-\frac{1}{8a}(\omega - \omega')^2\right),
\end{equation}
where $a = 0.5$ controls the smoothness.

\textbf{Results.} Using rank-15 factorization with a 3-layer [64, 64, 64] network:
\begin{itemize}
\item \textbf{Error: 46\%} relative $L^2$ norm (best achieved)
\item \textbf{Training: 1000 epochs}, converged to loss $-43.90$
\item \textbf{Sampling: Successful} ✓ (no Cholesky failures!)
\item \textbf{Visual match:} Learned spectral density closely resembles true density (see Figure~\ref{fig:silverman})
\end{itemize}

\textbf{Comparison to Baselines:}
\begin{itemize}
\item Direct MLP (no factorization): 111\% error, sampling fails
\item Sampling-based covariance: $>$2000\% error, high gradient noise
\item Moment matching loss: $\sim$2000\% error, unstable training
\end{itemize}

\textbf{[TODO: Add Figure 1 - Silverman results showing learned vs true spectral density, samples, training curves]}

\subsection{Ablation Studies}

\textbf{[TO BE COMPLETED - from PLAN.md Phase 1.3]}

\textbf{Effect of Rank:} Test $r \in \{5, 10, 15, 20, 30\}$
\begin{itemize}
\item Expected: Error decreases with rank up to $r \approx 15$, then plateaus
\item Optimal rank depends on kernel complexity
\end{itemize}

\textbf{Effect of Network Size:} Test hidden dims $\in \{[32,32], [64,64], [128,128]\}$
\begin{itemize}
\item Expected: Moderate size ([64,64]) works best
\item Larger networks risk overfitting
\end{itemize}

\textbf{Effect of $M$ (number of frequencies):}
\begin{itemize}
\item Expected: Convergence to true error as $M$ increases
\item Diminishing returns beyond $M = 50$ for 1D
\end{itemize}

\subsection{Real-World Experiments}

\textbf{[TO BE COMPLETED - from PLAN.md Phase 2]}

\textbf{Dataset: Mauna Loa CO₂}
\begin{itemize}
\item $n = 500$ observations, known nonstationary trends
\item Compare: F-SDN vs standard GP vs variational GP
\item Metrics: Test log-likelihood, RMSE, calibration
\end{itemize}

\textbf{[TODO: Add comparison table and plots]}

\section{Theory}

\subsection{Positive Definiteness Guarantee}

\begin{theorem}[Factorization Ensures PSD]
Let $f: \R^d \to \R^r$ be any function. Then $s(\omega, \omega') = f(\omega)^\top f(\omega')$ is positive semi-definite.
\end{theorem}

\begin{proof}
For any $M \in \mathbb{N}$ and $\{\alpha_i\} \in \C^M$, consider:
\begin{align}
\sum_{i,j=1}^M \overline{\alpha_i} s(\omega_i, \omega_j) \alpha_j
&= \sum_{i,j=1}^M \overline{\alpha_i} (f(\omega_i)^\top f(\omega_j)) \alpha_j \\
&= \left\langle \sum_{i=1}^M \alpha_i f(\omega_i), \sum_{j=1}^M \alpha_j f(\omega_j) \right\rangle \\
&= \left\| \sum_{i=1}^M \alpha_i f(\omega_i) \right\|^2 \geq 0.
\end{align}
Thus $s(\omega, \omega')$ satisfies the definition of a positive semi-definite kernel. \qed
\end{proof}

\textbf{Remark.} This holds for \emph{any} function $f$, including neural networks with arbitrary activations. The PSD property is purely a consequence of the factorized structure.

\subsection{Approximation Bounds}

\textbf{[TO BE COMPLETED - from PLAN.md Phase 3.1]}

\begin{itemize}
\item Under what conditions does $s_{\text{learned}} \to s_{\text{true}}$ as $n \to \infty$?
\item Can we bound $\|s_{\text{learned}} - s_{\text{true}}\|$ as function of $(n, M, r)$?
\item Connection to universal approximation theorems for neural networks
\end{itemize}

\section{Discussion}

\subsection{Why Factorization Works}

The success of our low-rank factorization can be understood from multiple perspectives:

\textbf{1. Spectral Efficiency.} Real-world nonstationary processes often have \emph{low effective rank} in the frequency domain---most covariance structure can be captured by a small number of dominant eigenmodes. Our explicit rank-$r$ parametrization enforces this inductive bias.

\textbf{2. Optimization Landscape.} The factorization removes the hard PSD constraint, simplifying the optimization to unconstrained learning of $f(\omega)$. This eliminates saddle points and ill-conditioning that arise when enforcing PSD post-hoc.

\textbf{3. Generalization.} Low-rank structure acts as implicit regularization, preventing overfitting to spurious high-frequency patterns in the training data.

\subsection{Identifiability}

An interesting observation: the learned spectral density $s_{\text{learned}}(\omega, \omega')$ may \emph{look visually different} from the true $s_{\text{true}}$, yet produce functionally equivalent samples. This suggests that multiple spectral densities can explain the same posterior observations.

\textbf{Open question:} Is $s(\omega, \omega')$ \emph{identifiable} from finite observations? Or is there a family of equivalent spectral densities?

\subsection{Limitations}

\begin{itemize}
\item \textbf{Rank selection:} Currently chosen via cross-validation. Can we develop principled rank selection criteria?
\item \textbf{High dimensions:} Scaling to $d > 3$ may require structured factorizations (e.g., tensor decompositions).
\item \textbf{Interpretability:} The learned $f(\omega)$ is a black-box MLP. Can we design interpretable architectures?
\end{itemize}

\section{Conclusion}

We introduced \textbf{Factorized Spectral Density Networks}, a principled method for learning nonstationary Gaussian processes from data. By parametrizing the spectral density $s(\omega, \omega')$ through a low-rank neural factorization, we achieve three key benefits: (1) guaranteed positive definiteness, (2) stable training via deterministic loss, and (3) efficient $O(Mn)$ inference via Neural Fourier Features. Our experiments demonstrate 46\% relative error on synthetic kernels with reliable sampling, substantially outperforming baseline approaches.

\textbf{Future directions} include: extending to multi-output GPs, incorporating physics-informed constraints in $s(\omega, \omega')$, and developing theoretical guarantees for approximation error and sample complexity. We believe our spectral perspective opens new avenues for scalable, interpretable nonstationary GP inference.

% \section*{Broader Impact}
% [TO BE COMPLETED if accepted]

\section*{Acknowledgments}
We thank [to be added after de-anonymization].

% Bibliography
\bibliographystyle{plain}
% \bibliography{references}

% For now, inline key references until we create references.bib
\begin{thebibliography}{99}

\bibitem{rasmussen2006gaussian}
Carl Edward Rasmussen and Christopher KI Williams.
\newblock Gaussian processes for machine learning.
\newblock MIT press, 2006.

\bibitem{bochner1959lectures}
Salomon Bochner.
\newblock Lectures on Fourier integrals.
\newblock Princeton University Press, 1959.

\bibitem{silverman1957classes}
Richard A Silverman.
\newblock Locally stationary random processes.
\newblock IRE Transactions on Information Theory, 1957.

\bibitem{paciorek2004nonstationary}
Christopher Paciorek and Mark Schervish.
\newblock Nonstationary covariance functions for Gaussian process regression.
\newblock NIPS, 2004.

\bibitem{gibbs1997bayesian}
Mark N Gibbs.
\newblock Bayesian Gaussian processes for regression and classification.
\newblock PhD thesis, University of Cambridge, 1997.

\bibitem{wilson2013gaussian}
Andrew G Wilson and Ryan P Adams.
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock ICML, 2013.

\bibitem{wilson2016deep}
Andrew G Wilson et al.
\newblock Deep kernel learning.
\newblock AISTATS, 2016.

\bibitem{garnelo2018neural}
Marta Garnelo et al.
\newblock Neural processes.
\newblock ICML Workshop, 2018.

\bibitem{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock NIPS, 2007.

\bibitem{heinonen2016non}
Markus Heinonen et al.
\newblock Non-stationary Gaussian process regression with Hamiltonian Monte Carlo.
\newblock AISTATS, 2016.

\bibitem{jawaid2024thesis}
Arsalan Jawaid.
\newblock Flexible Gaussian processes via harmonizable and regular spectral representations.
\newblock PhD thesis, 2024.

\bibitem{loeve1978probability}
Michel Loève.
\newblock Probability theory II.
\newblock Springer, 1978.

\end{thebibliography}

\end{document}
