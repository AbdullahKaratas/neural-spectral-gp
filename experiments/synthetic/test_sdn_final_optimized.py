"""
FINAL OPTIMIZED SDN Training - Going for the Gold! üèÜ

This is our best shot:
- BIG network (128√ó128√ó128)
- Posterior loss (deterministic!)
- Long training (1000 epochs)
- Adaptive LR scheduling
- Careful regularization tuning

Authors: Abdullah Karatas, Arsalan Jawaid
"""

import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from nsgp.models.sdn import SpectralDensityNetwork
from nsgp.models.nffs import NFFs


def silverman_spectral_density(omega1, omega2, a=0.5):
    """Silverman's spectral density (ground truth)."""
    if omega1.dim() == 1:
        omega1 = omega1.unsqueeze(-1)
    if omega2.dim() == 1:
        omega2 = omega2.unsqueeze(-1)

    omega_sum = (omega1 + omega2) / 2.0
    omega_diff = omega1 - omega2

    prefactor = 1.0 / (4.0 * np.pi * a)
    exp1 = torch.exp(-1.0 / (2.0 * a) * omega_sum**2)
    exp2 = torch.exp(-1.0 / (8.0 * a) * omega_diff**2)

    return (prefactor * exp1 * exp2).squeeze()


def generate_synthetic_data(n_train=50, a=0.5, noise_std=0.1, seed=42):
    """Generate synthetic training data from Silverman GP."""
    torch.manual_seed(seed)

    X_train = torch.linspace(-3, 3, n_train).unsqueeze(-1)

    def spectral_density_fn(w1, w2):
        return silverman_spectral_density(w1, w2, a=a)

    nffs = NFFs(
        spectral_density=spectral_density_fn,
        n_features=100,
        omega_max=10.0,
        input_dim=1
    )

    y_train = nffs.simulate(X_train, n_samples=1, seed=seed).squeeze()
    y_train = y_train + noise_std * torch.randn_like(y_train)
    y_train = y_train - y_train.mean()

    return X_train, y_train


def train_sdn_optimized(
    sdn,
    X_train,
    y_train,
    epochs=1000,
    initial_lr=1e-2,
    noise_std=0.1,
    patience=150
):
    """
    Optimized training with all the bells and whistles.
    """
    y_train = y_train - y_train.mean()

    # Adam optimizer
    optimizer = torch.optim.Adam(sdn.parameters(), lr=initial_lr)

    # Cosine annealing with warm restarts
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=100,  # Restart every 100 epochs
        T_mult=2,  # Double the period after each restart
        eta_min=initial_lr / 100
    )

    # Early stopping
    best_loss = float('inf')
    best_state = None
    patience_counter = 0

    losses = []

    print("FINAL OPTIMIZED TRAINING:")
    print(f"  Network: {sum(p.numel() for p in sdn.parameters()):,} parameters")
    print(f"  Epochs: {epochs}")
    print(f"  Initial LR: {initial_lr}")
    print(f"  LR scheduler: Cosine Annealing with Warm Restarts")
    print(f"  Early stopping patience: {patience}")
    print()

    for epoch in range(epochs):
        optimizer.zero_grad()

        # Posterior loss (deterministic!)
        data_loss = sdn.posterior_mean_loss(X_train, y_train, noise_var=noise_std**2)

        # Regularization (carefully tuned)
        smooth_penalty = sdn.spectral_smoothness_penalty()
        rank_penalty = sdn.low_rank_penalty()

        # Total loss with tuned weights
        loss = data_loss + 0.1 * smooth_penalty + 0.005 * rank_penalty

        # Backpropagation
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(sdn.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

        # Track
        losses.append(loss.item())
        current_lr = optimizer.param_groups[0]['lr']

        # Early stopping
        if loss.item() < best_loss:
            best_loss = loss.item()
            best_state = {k: v.cpu().clone() for k, v in sdn.state_dict().items()}
            patience_counter = 0
        else:
            patience_counter += 1

        # Print progress
        if epoch % 100 == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.4f} | "
                  f"Data: {data_loss.item():.4f} | LR: {current_lr:.6f} | "
                  f"Best: {best_loss:.4f}")

        # Early stopping
        if patience_counter >= patience:
            print(f"\n‚úì Early stopping at epoch {epoch} (no improvement for {patience} epochs)")
            break

    # Restore best model
    if best_state is not None:
        sdn.load_state_dict(best_state)
        print(f"\n‚úì Restored best model (loss: {best_loss:.4f})")

    return losses


def test_final_optimized():
    print("=" * 70)
    print("FINAL OPTIMIZED SDN TRAINING - Going for the Gold! üèÜ")
    print("=" * 70)

    # Parameters
    a = 0.5
    n_train = 50
    noise_std = 0.1
    n_features = 50
    omega_max = 8.0

    print(f"\nSetup:")
    print(f"  Kernel: Silverman (a={a})")
    print(f"  n_train: {n_train}")
    print(f"  noise_std: {noise_std}")

    # Generate data
    print(f"\nGenerating synthetic data...")
    X_train, y_train = generate_synthetic_data(
        n_train=n_train,
        a=a,
        noise_std=noise_std,
        seed=42
    )
    print(f"‚úì Generated {len(X_train)} training points")

    # Initialize MEDIUM SDN (sweet spot!)
    print(f"\nInitializing MEDIUM SDN (optimal size)...")
    sdn = SpectralDensityNetwork(
        input_dim=1,
        hidden_dims=[64, 64, 64],  # Medium - works best!
        n_features=n_features,
        omega_max=omega_max,
        activation='elu'
    )
    n_params = sum(p.numel() for p in sdn.parameters())
    print(f"‚úì SDN initialized with {n_params:,} parameters")

    # Train
    print(f"\n" + "-" * 70)
    losses = train_sdn_optimized(
        sdn=sdn,
        X_train=X_train,
        y_train=y_train,
        epochs=1000,
        initial_lr=1e-2,
        noise_std=noise_std,
        patience=150
    )
    print("-" * 70)

    # Evaluate
    print(f"\nEvaluating learned spectral density...")
    omega_grid = torch.linspace(-omega_max/2, omega_max/2, 50)
    W1, W2 = torch.meshgrid(omega_grid, omega_grid, indexing='ij')

    S_true = silverman_spectral_density(
        W1.flatten().unsqueeze(-1),
        W2.flatten().unsqueeze(-1),
        a=a
    ).reshape(W1.shape)

    with torch.no_grad():
        S_learned = sdn.forward(
            W1.flatten().unsqueeze(-1),
            W2.flatten().unsqueeze(-1)
        ).reshape(W1.shape)

    relative_error = torch.norm(S_learned - S_true) / torch.norm(S_true)
    print(f"  Relative L2 error: {relative_error.item():.4f} ({relative_error.item()*100:.1f}%)")

    # Generate samples (try, but may fail if S not PD)
    print(f"\nGenerating samples...")
    X_test = torch.linspace(-4, 4, 100).unsqueeze(-1)

    try:
        samples_learned = sdn.simulate(X_test, n_samples=5, seed=123)
        sampling_worked = True
    except RuntimeError as e:
        print(f"  ‚ö†Ô∏è Sampling failed (spectral matrix not PD): {str(e)[:80]}")
        print(f"  ‚Üí Skipping sample generation, showing spectral density only")
        sampling_worked = False
        samples_learned = None

    def spectral_density_fn_true(w1, w2):
        return silverman_spectral_density(w1, w2, a=a)

    nffs_true = NFFs(
        spectral_density=spectral_density_fn_true,
        n_features=n_features,
        omega_max=omega_max,
        input_dim=1
    )
    samples_true = nffs_true.simulate(X_test, n_samples=5, seed=123)

    # Visualization
    print(f"\nCreating visualizations...")
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # Row 1: Samples
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.scatter(X_train.numpy(), y_train.numpy(), c='red', s=40, alpha=0.7,
                edgecolors='darkred', linewidths=1.5, label='Training data')
    ax1.set_xlabel('x', fontsize=12)
    ax1.set_ylabel('y', fontsize=12)
    ax1.set_title('Training Data', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    ax2 = fig.add_subplot(gs[0, 1])
    if sampling_worked and samples_learned is not None:
        for i in range(samples_learned.shape[0]):
            ax2.plot(X_test.numpy(), samples_learned[i].numpy(), alpha=0.7, linewidth=2)
        ax2.scatter(X_train.numpy(), y_train.numpy(), c='red', s=30, alpha=0.6, zorder=10)
        ax2.set_title('Samples from Learned Prior', fontsize=14, fontweight='bold')
    else:
        ax2.text(0.5, 0.5, 'Sampling Failed\n(S not PD)', ha='center', va='center',
                 transform=ax2.transAxes, fontsize=16, color='red')
        ax2.set_title('Samples from Learned Prior (FAILED)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('x', fontsize=12)
    ax2.set_ylabel('Z(x)', fontsize=12)
    ax2.grid(True, alpha=0.3)

    ax3 = fig.add_subplot(gs[0, 2])
    for i in range(samples_true.shape[0]):
        ax3.plot(X_test.numpy(), samples_true[i].numpy(), alpha=0.7, linewidth=2)
    ax3.scatter(X_train.numpy(), y_train.numpy(), c='red', s=30, alpha=0.6, zorder=10)
    ax3.set_xlabel('x', fontsize=12)
    ax3.set_ylabel('Z(x)', fontsize=12)
    ax3.set_title('Samples from True Prior', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)

    # Row 2: Spectral densities
    ax4 = fig.add_subplot(gs[1, 0])
    im1 = ax4.contourf(W1.numpy(), W2.numpy(), S_true.numpy(), levels=20, cmap='viridis')
    ax4.set_xlabel('œâ', fontsize=12)
    ax4.set_ylabel("œâ'", fontsize=12)
    ax4.set_title("True Spectral Density s(œâ,œâ')", fontsize=14, fontweight='bold')
    plt.colorbar(im1, ax=ax4)

    ax5 = fig.add_subplot(gs[1, 1])
    im2 = ax5.contourf(W1.numpy(), W2.numpy(), S_learned.numpy(), levels=20, cmap='viridis')
    ax5.set_xlabel('œâ', fontsize=12)
    ax5.set_ylabel("œâ'", fontsize=12)
    ax5.set_title("Learned Spectral Density (BIG NET)", fontsize=14, fontweight='bold')
    plt.colorbar(im2, ax=ax5)

    ax6 = fig.add_subplot(gs[1, 2])
    error = torch.abs(S_learned - S_true)
    im3 = ax6.contourf(W1.numpy(), W2.numpy(), error.numpy(), levels=20, cmap='Reds')
    ax6.set_xlabel('œâ', fontsize=12)
    ax6.set_ylabel("œâ'", fontsize=12)
    ax6.set_title(f"Absolute Error\n(Relative L2: {relative_error.item():.3f})",
                  fontsize=14, fontweight='bold')
    plt.colorbar(im3, ax=ax6)

    # Row 3: Training curves
    ax7 = fig.add_subplot(gs[2, :])
    ax7.plot(losses, 'b-', linewidth=2.5, alpha=0.8)
    ax7.set_xlabel('Epoch', fontsize=12)
    ax7.set_ylabel('Total Loss', fontsize=12)
    ax7.set_title('Training Loss (Final Optimized Run)', fontsize=14, fontweight='bold')
    ax7.grid(True, alpha=0.3)
    ax7.axhline(losses[-1], color='r', linestyle='--', linewidth=2, alpha=0.7,
                label=f'Final: {losses[-1]:.2f}')
    ax7.axhline(min(losses), color='g', linestyle='--', linewidth=2, alpha=0.7,
                label=f'Best: {min(losses):.2f}')
    ax7.legend(fontsize=11)

    plt.suptitle('FINAL OPTIMIZED: Big Network (128¬≥) + Posterior Loss + 1000 Epochs',
                 fontsize=16, fontweight='bold')

    output_path = Path(__file__).parent / "sdn_final_optimized_results.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"‚úì Figure saved to: {output_path}")

    plt.show()

    # Summary
    print(f"\n" + "=" * 70)
    print("FINAL RESULTS")
    print("=" * 70)
    print(f"Network: {n_params:,} parameters (128√ó128√ó128)")
    print(f"Training epochs: {len(losses)}")
    print(f"Final loss: {losses[-1]:.4f}")
    print(f"Best loss: {min(losses):.4f}")
    print(f"Spectral density error: {relative_error.item()*100:.1f}%")
    print()

    if relative_error.item() < 0.5:
        print("üèÜ OUTSTANDING! Error < 50%")
    elif relative_error.item() < 1.0:
        print("üéâ EXCELLENT! Error < 100%")
    elif relative_error.item() < 2.0:
        print("‚úÖ GOOD! Error < 200%")
    else:
        print("‚úì Decent progress, but still room for improvement")

    print("=" * 70)


if __name__ == "__main__":
    test_final_optimized()
