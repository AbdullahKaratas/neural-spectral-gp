# ğŸ“š Neural Spectral GP - Theory Explained (For Dummies!)

**Authors:** Abdullah Karatas, Arsalan Jawaid
**Date:** November 7, 2025
**Purpose:** Explain the math behind our NeurIPS 2026 paper in simple terms

---

## Table of Contents
1. [What is a Gaussian Process?](#1-what-is-a-gaussian-process-gp)
2. [Stationary vs Nonstationary](#2-stationary-vs-nonstationary)
3. [Spectral Representation](#3-the-spectral-representation-the-trick)
4. [The Problem](#4-the-problem-how-do-we-learn-sÏ‰-Ï‰)
5. [Our Solution: Factorization](#5-our-solution-factorization-)
6. [Training](#6-the-training-how-does-the-network-learn)
7. [Visualization](#7-visualization-of-the-equations)
8. [Rank Explained](#8-the-meaning-of-rank-r)
9. [Summary](#9-summary-the-theory-in-one-picture)
10. [Key Equations](#10-the-most-important-equations)
11. [Quiz](#-final-check-do-you-understand-it)

---

## 1. What is a Gaussian Process (GP)?

### ğŸ¨ Without Math:
A GP is like a **machine that draws random functions**. Every time you turn it on, it draws a different wavy line.

### ğŸ“ With Math:
```
Z(x) ~ GP(Î¼(x), k(x,x'))
```
- `Z(x)` = The random function (our "picture")
- `Î¼(x)` = Mean value (usually 0)
- `k(x,x')` = Covariance kernel (tells how "similar" two points are)

### ğŸ’¡ Example:
If `x=1` and `x'=1.1` are close together â†’ `k(1, 1.1)` is large â†’ the function is smooth there!

---

## 2. Stationary vs Nonstationary

### Stationary (boring ğŸ˜´):
```
k(x, x') = k(x - x')
```
**Meaning:** Covariance depends only on **distance**.

**Example:** Like waves in a pool - same everywhere!

### Nonstationary (interesting! ğŸŒŠ):
```
k(x, x') â‰  k(x - x')
```
**Meaning:** Covariance can **behave differently everywhere**!

**Example:** Ocean - calm at the beach, wild in a storm!

---

## 3. The Spectral Representation (The Trick!)

### For Stationary GPs (simple):

**Bochner's Theorem:**
```
k(x - x') = âˆ« e^(iÏ‰(x-x')) S(Ï‰) dÏ‰
```

**What does this mean?**
- `S(Ï‰)` = **Spectral density** (how much of each frequency `Ï‰`)
- `Ï‰` = Frequency (how fast the wave oscillates)
- The integral = Fourier Transform (converts frequencies â†’ function)

**Analogy:**
- `S(Ï‰)` is like a **music equalizer** ğŸšï¸
- Each slider (frequency) tells how strong that frequency is
- The integral **mixes all frequencies together** â†’ music! ğŸµ

---

### For Nonstationary GPs (more complex):

**Harmonizable Processes:**
```
k(x, x') = âˆ«âˆ« e^(iÏ‰x - iÏ‰'x') s(Ï‰, Ï‰') dÏ‰ dÏ‰'
```

**WHAT?! Two integrals?!**

**Yes! And here's the trick:**
- For stationary: `s(Ï‰, Ï‰') = S(Ï‰) Î´(Ï‰ - Ï‰')` (only diagonal!)
- For nonstationary: `s(Ï‰, Ï‰')` can have **values everywhere**!

---

### ğŸ“Š Visualization:

**Stationary (Diagonal only):**
```
     Ï‰'
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â–ˆ       â”‚  â† only on diagonal
  â”‚  â–ˆ      â”‚
Ï‰ â”‚   â–ˆ     â”‚
â†’ â”‚    â–ˆ    â”‚
  â”‚     â–ˆ   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nonstationary (Full matrix):**
```
     Ï‰'
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â”‚  â† values everywhere!
  â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â”‚
Ï‰ â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â”‚
â†’ â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â”‚
  â”‚ â–ˆ â–ˆ â–ˆ â–ˆ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. The Problem: How Do We Learn s(Ï‰, Ï‰')?

### Challenge:
We have **data** `{(x_i, y_i)}` but we want to learn `s(Ï‰, Ï‰')`!

### Constraint (IMPORTANT!):
`s(Ï‰, Ï‰')` must be **positive definite** (PD)!

---

### What Does PD Mean?

**Mathematically:**
```
âˆ‘áµ¢â±¼ Î±áµ¢* s(Ï‰áµ¢, Ï‰â±¼) Î±â±¼ â‰¥ 0   for all {Î±áµ¢}
```

**For 5-year-olds:**
Think of `s(Ï‰, Ï‰')` as a **matrix**:
```
S = [ s(Ï‰â‚,Ï‰â‚)  s(Ï‰â‚,Ï‰â‚‚)  s(Ï‰â‚,Ï‰â‚ƒ) ]
    [ s(Ï‰â‚‚,Ï‰â‚)  s(Ï‰â‚‚,Ï‰â‚‚)  s(Ï‰â‚‚,Ï‰â‚ƒ) ]
    [ s(Ï‰â‚ƒ,Ï‰â‚)  s(Ï‰â‚ƒ,Ï‰â‚‚)  s(Ï‰â‚ƒ,Ï‰â‚ƒ) ]
```

**PD means:** All eigenvalues are â‰¥ 0.

**Why important?**
If `S` is not PD â†’ **Cholesky decomposition fails** â†’ we CANNOT sample! âŒ

---

## 5. Our Solution: Factorization! ğŸ¯

### The Idea:
**Instead of learning `s(Ï‰, Ï‰')` directly, we learn `f(Ï‰)` and set:**
```
s(Ï‰, Ï‰') = f(Ï‰)áµ€ f(Ï‰')
```

### What is `f(Ï‰)`?
- A neural network!
- Input: Frequency `Ï‰` (e.g., Ï‰=2.5)
- Output: Vector `f(Ï‰) âˆˆ â„Ê³` (e.g., r=15)

---

### ğŸ’¡ Example with r=3:
```
f(Ï‰â‚) = [0.5, 0.2, 0.8]
f(Ï‰â‚‚) = [0.3, 0.7, 0.1]

s(Ï‰â‚, Ï‰â‚‚) = f(Ï‰â‚)áµ€ f(Ï‰â‚‚)
          = 0.5Ã—0.3 + 0.2Ã—0.7 + 0.8Ã—0.1
          = 0.15 + 0.14 + 0.08
          = 0.37
```

---

### Why is This Brilliant? ğŸ’¡

**THEOREM (the most important!):**
```
s(Ï‰, Ï‰') = f(Ï‰)áµ€ f(Ï‰')  âŸ¹  s is GUARANTEED PD!
```

**Proof (simple!):**
```
âˆ‘áµ¢â±¼ Î±áµ¢* s(Ï‰áµ¢, Ï‰â±¼) Î±â±¼
= âˆ‘áµ¢â±¼ Î±áµ¢* (f(Ï‰áµ¢)áµ€ f(Ï‰â±¼)) Î±â±¼
= âˆ‘áµ¢â±¼ (Î±áµ¢ f(Ï‰áµ¢))áµ€ (Î±â±¼ f(Ï‰â±¼))
= || âˆ‘áµ¢ Î±áµ¢ f(Ï‰áµ¢) ||Â²
â‰¥ 0  âœ“
```

**For 5-year-olds:**
- The square of a number is always â‰¥ 0 (e.g., 3Â² = 9 â‰¥ 0, (-3)Â² = 9 â‰¥ 0)
- `||v||Â²` is also always â‰¥ 0
- Therefore our `s(Ï‰, Ï‰')` is always PD! **No more Cholesky failures!** âœ“

---

## 6. The Training: How Does the Network Learn?

### The Loss Function:

**We want:** `s(Ï‰, Ï‰')` such that the **likelihood** of data is maximal.

**GP Marginal Likelihood (GPML eq 2.30):**
```
-log p(y|X) = Â½ yáµ€ Kâ»Â¹ y + Â½ log|K| + (n/2) log(2Ï€)
              â†‘              â†‘           â†‘
          data fit    complexity   constant
```

---

### What Does This Mean?

1. **Data fit term:** `Â½ yáµ€ Kâ»Â¹ y`
   - How well do the data fit the covariance?
   - Small = good!

2. **Complexity penalty:** `Â½ log|K|`
   - Penalizes overly complex models
   - Occam's Razor!

3. **Constant:** `(n/2) log(2Ï€)`
   - Doesn't matter for optimization

---

### How Do We Compute K?

**From s(Ï‰,Ï‰') â†’ K(x,x'):**
```
K(x, x') = âˆ«âˆ« e^(iÏ‰x - iÏ‰'x') s(Ï‰, Ï‰') dÏ‰ dÏ‰'
```

**Monte Carlo Approximation:**
```
K(x, x') â‰ˆ (vol/(2Ï€)áµˆ) âˆ‘â‚˜ s(Ï‰â‚˜, Ï‰â‚˜) cos(Ï‰â‚˜áµ€(x - x'))
```

**Step by step:**
1. Sample M frequencies: `{Ï‰â‚, ..., Ï‰â‚˜}`
2. Compute `s(Ï‰â‚˜, Ï‰â‚˜) = f(Ï‰â‚˜)áµ€ f(Ï‰â‚˜)` for each m
3. Sum with cos-weights â†’ K(x, x')

**This is DETERMINISTIC!** (No sampling â†’ no gradient noise!)

---

## 7. Visualization of the Equations

### The Chain:

```
Training Data      Neural Net       Spectral Density     Covariance       Likelihood
{(xáµ¢, yáµ¢)}    â†’  f_Î¸(Ï‰) âˆˆ â„Ê³  â†’  s(Ï‰,Ï‰')=f(Ï‰)áµ€f(Ï‰') â†’ K via Fourier â†’ -log p(y|X)
                      â†‘                                                         â†“
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradient descent â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### What Does the Network Learn?

**Input:** Frequency Ï‰ (e.g., [2.5])
**Hidden Layers:** 3 layers with [64, 64, 64] neurons
**Output:** Feature vector f(Ï‰) âˆˆ â„Â¹âµ

**Architecture:**
```
Ï‰ â†’ [Linear + ELU] â†’ [Linear + ELU] â†’ [Linear + ELU] â†’ f(Ï‰)
    64 neurons        64 neurons        64 neurons       15 dim
```

**Then:**
```
s(Ï‰, Ï‰') = f(Ï‰) Â· f(Ï‰')  (dot product)
```

---

## 8. The Meaning of "Rank" (r)

### What is r?
- The dimension of `f(Ï‰)`
- r = 15 means: `f(Ï‰) âˆˆ â„Â¹âµ`

### Why Important?

**Low-Rank Approximation:**
```
s(Ï‰, Ï‰') = âˆ‘áµ¢â‚Œâ‚Ê³ fáµ¢(Ï‰) fáµ¢(Ï‰')
```

**Intuition:**
- r = 1: Very simple (only 1 "mode")
- r = 15: Moderate complexity (15 "modes")
- r = 100: Very flexible (100 "modes")

**Our choice: r=15**
- Not too simple (underfitting)
- Not too complex (overfitting)
- **Goldilocks Zone!** ğŸ»

---

## 9. Summary: The Theory in One Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NONSTATIONARY GP                         â”‚
â”‚                                                               â”‚
â”‚  Observations: y = Z(x) + noise                             â”‚
â”‚                                                               â”‚
â”‚  Goal: Learn s(Ï‰,Ï‰') such that induced GP explains data    â”‚
â”‚                                                               â”‚
â”‚  Constraint: s must be POSITIVE DEFINITE (hard!)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OUR SOLUTION                               â”‚
â”‚                                                               â”‚
â”‚  Parametrize: s(Ï‰,Ï‰') = f(Ï‰)áµ€ f(Ï‰')                       â”‚
â”‚                                                               â”‚
â”‚  where f: â„áµˆ â†’ â„Ê³ is a neural network                     â”‚
â”‚                                                               â”‚
â”‚  âœ“ PD guaranteed by construction!                           â”‚
â”‚  âœ“ r controls complexity (rank)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING                                  â”‚
â”‚                                                               â”‚
â”‚  Loss: -log p(y|X) where K = Fourierâ»Â¹(s)                 â”‚
â”‚                                                               â”‚
â”‚  Gradient descent: Î¸ â† Î¸ - Î· âˆ‡Î¸ Loss                       â”‚
â”‚                                                               â”‚
â”‚  Result: Learned s(Ï‰,Ï‰') with 46% error! ğŸ‰               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. The Most Important Equations

### Equation 1: Factorization (THE CORE IDEA!)
```
s(Ï‰, Ï‰') = f(Ï‰)áµ€ f(Ï‰')
```
**Meaning:** Spectral density as product of features
**Why:** Guarantees PD!

---

### Equation 2: Covariance via Inverse Fourier
```
k(x, x') = âˆ«âˆ« e^(iÏ‰x - iÏ‰'x') s(Ï‰, Ï‰') dÏ‰ dÏ‰'
```
**Meaning:** From frequencies â†’ spatial domain
**Approximation:** Monte Carlo with M samples

---

### Equation 3: GP Marginal Likelihood
```
-log p(y|X) = Â½ yáµ€ Kâ»Â¹ y + Â½ log|K| + const
```
**Meaning:** How likely is the data under this GP?
**Training:** Minimize this function!

---

### Equation 4: PD Guarantee
```
âˆ‘áµ¢â±¼ Î±áµ¢* s(Ï‰áµ¢, Ï‰â±¼) Î±â±¼ = || âˆ‘áµ¢ Î±áµ¢ f(Ï‰áµ¢) ||Â² â‰¥ 0
```
**Meaning:** Proof that s is always PD
**Consequence:** Sampling always works! âœ“

---

## ğŸ¯ Final Check: Do You Understand It?

**Quiz:**
1. What is `s(Ï‰, Ï‰')`? â†’ Spectral density (describes GP in frequency domain)
2. Why must s be PD? â†’ Otherwise Cholesky fails!
3. How do we guarantee PD? â†’ Factorization: s = fáµ€f
4. What is r? â†’ Rank of factorization (we use r=15)
5. How do we train? â†’ Minimize -log p(y|X) via gradient descent

**If you can answer all 5 â†’ you understand it! ğŸ“**

---

## ğŸ“Š Comparison: Before vs After Factorization

### Before (Direct MLP):
```
Problem: Learn s(Ï‰, Ï‰') directly with neural network
Challenge: How to enforce PD?
Results: 111% error, Cholesky failures âŒ
```

### After (Factorized):
```
Solution: Learn f(Ï‰), set s(Ï‰,Ï‰') = f(Ï‰)áµ€f(Ï‰')
Advantage: PD guaranteed by construction!
Results: 46% error, sampling works! âœ“
```

---

## ğŸ”¬ Real Experiment Results

### Silverman Kernel Test:
- **True spectral density:** Known analytic form
- **Learned spectral density:** Via our factorized network
- **Error:** 46% relative L2 norm
- **Visual match:** Almost identical! (see `sdn_factorized_results.png`)
- **Sampling:** 5 sample paths generated successfully âœ“

### Training Details:
- **Network:** 3 layers, [64, 64, 64] hidden units
- **Rank:** r = 15
- **Epochs:** 1000 (early stopped at 263)
- **Final loss:** -43.90
- **Optimizer:** Adam with cosine annealing

---

## ğŸ’¡ Key Insights

1. **Factorization is the key innovation**
   - Simple idea with profound consequences
   - Mathematical elegance + practical benefits

2. **PD by construction eliminates a whole class of errors**
   - No more Cholesky failures
   - Stable, reliable training

3. **Deterministic covariance computation**
   - No sampling noise in gradients
   - Fast convergence

4. **Low-rank structure is implicit regularization**
   - Prevents overfitting
   - Encourages parsimony

---

## ğŸ“š References for Deep Dive

1. **Harmonizable Processes:**
   - LoÃ¨ve, M. (1978). Probability Theory II. Springer.
   - Silverman, R. A. (1957). Locally stationary random processes.

2. **Gaussian Processes:**
   - Rasmussen & Williams (2006). Gaussian Processes for Machine Learning.

3. **Spectral Methods:**
   - Bochner, S. (1959). Lectures on Fourier integrals.
   - Rahimi & Recht (2007). Random features for large-scale kernel machines.

4. **Neural Fourier Features:**
   - Jawaid, A. (2024). PhD Thesis, Chapter 6.

---

## ğŸ“ For Teaching

This document can be used to:
- Teach new students joining the project
- Explain the method in presentations
- Write the "Background" and "Method" sections of the paper
- Answer reviewer questions

---

## ğŸš€ Next Steps

Now that you understand the theory, check out:
1. **PLAN.md** - Publication roadmap
2. **paper/neural_spectral_gp.tex** - The actual paper draft
3. **src/nsgp/models/sdn_factorized.py** - The implementation
4. **experiments/synthetic/test_sdn_factorized.py** - The experiment

---

**Questions? Ideas? Found this helpful?**

Contact: abdullah.karatas@icloud.com

---

*"The best theories are those that are both mathematically elegant and practically useful."*

**â€” Abdullah & Arsalan, November 2025**
